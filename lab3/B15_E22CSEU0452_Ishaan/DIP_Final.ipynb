{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWfmWdtYa8D1",
        "outputId": "97d9e1cd-e915-47e9-d8e5-47c6fc8b8f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "import numpy as np\n",
        "import imageio\n",
        "import random\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from operator import add\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score"
      ],
      "metadata": {
        "id": "FEewpYX4a-d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation on CHASE"
      ],
      "metadata": {
        "id": "Jp7RAJ40geQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Create a directory \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path):\n",
        "    train_x = sorted(glob(os.path.join(path, \"training\", \"images\", \"*.tif\")))\n",
        "    train_y = sorted(glob(os.path.join(path, \"training\", \"1st_manual\", \"*.gif\")))\n",
        "\n",
        "    test_x = sorted(glob(os.path.join(path, \"test\", \"images\", \"*.tif\")))\n",
        "    test_y = sorted(glob(os.path.join(path, \"test\", \"1st_manual\", \"*.gif\")))\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "def augment_data(images, masks, save_path, augment=True):\n",
        "    size = (512, 512)\n",
        "\n",
        "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
        "        \"\"\" Extracting the name \"\"\"\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Reading image and mask \"\"\"\n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        y = imageio.mimread(y)[0]\n",
        "\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            aug = VerticalFlip(p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x2 = augmented[\"image\"]\n",
        "            y2 = augmented[\"mask\"]\n",
        "\n",
        "            aug = Rotate(limit=45, p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x3 = augmented[\"image\"]\n",
        "            y3 = augmented[\"mask\"]\n",
        "\n",
        "            X = [x, x1, x2, x3]\n",
        "            Y = [y, y1, y2, y3]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        index = 0\n",
        "        for i, m in zip(X, Y):\n",
        "            i = cv2.resize(i, size)\n",
        "            m = cv2.resize(m, size)\n",
        "\n",
        "            tmp_image_name = f\"{name}_{index}.png\"\n",
        "            tmp_mask_name = f\"{name}_{index}.png\"\n",
        "\n",
        "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
        "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
        "\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            index += 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    \"\"\" Load the data \"\"\"\n",
        "    data_path = \"/content/drive/MyDrive/DIP/\"\n",
        "    (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    \"\"\" Create directories to save the augmented data \"\"\"\n",
        "    create_dir(\"new_data/train/image/\")\n",
        "    create_dir(\"new_data/train/mask/\")\n",
        "    create_dir(\"new_data/test/image/\")\n",
        "    create_dir(\"new_data/test/mask/\")\n",
        "\n",
        "    \"\"\" Data augmentation \"\"\"\n",
        "    augment_data(train_x, train_y, \"new_data/train/\", augment=True)\n",
        "    augment_data(test_x, test_y, \"new_data/test/\", augment=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOqQNaIsf-YY",
        "outputId": "9eb5f727-74e6-4c79-c350-90140d3a0859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 20 - 20\n",
            "Test: 20 - 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:32<00:00,  1.62s/it]\n",
            "100%|██████████| 20/20 [00:30<00:00,  1.53s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset for training"
      ],
      "metadata": {
        "id": "PreO_Ep2d3JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path):\n",
        "\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
        "        image = image/255.0 ## (512, 512, 3)\n",
        "        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)\n",
        "        image = image.astype(np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        \"\"\" Reading mask \"\"\"\n",
        "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask/255.0   ## (512, 512)\n",
        "        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "o9SvbWiXcQm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "6fxO2RcXeCP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def give_acc(y_true, y_pred):\n",
        "    \"\"\" Ground truth \"\"\"\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "    y_true = y_true.reshape(-1)\n",
        "    \"\"\" Prediction \"\"\"\n",
        "    y_pred = y_pred.detach().cpu().numpy()\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
        "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "fg8XTXrvcQjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Loss functions"
      ],
      "metadata": {
        "id": "_MrvmQI-ezZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "\n",
        "        return Dice_BCE"
      ],
      "metadata": {
        "id": "lXd_Bjvpeyz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building U-Net model"
      ],
      "metadata": {
        "id": "AHQtQ1RMd066"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv = conv_block(in_c, out_c)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        p = self.pool(x)\n",
        "        return x, p\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "        self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.up(inputs)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class build_unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        self.e1 = encoder_block(3, 64)\n",
        "        self.e2 = encoder_block(64, 128)\n",
        "        self.e3 = encoder_block(128, 256)\n",
        "        self.e4 = encoder_block(256, 512)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        self.b = conv_block(512, 1024)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        self.d1 = decoder_block(1024, 512)\n",
        "        self.d2 = decoder_block(512, 256)\n",
        "        self.d3 = decoder_block(256, 128)\n",
        "        self.d4 = decoder_block(128, 64)\n",
        "\n",
        "        \"\"\" Classifier \"\"\"\n",
        "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        s1, p1 = self.e1(inputs)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        b = self.b(p4)\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        outputs = self.outputs(d4)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "hRjS47mQcQgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Training & Testing functions"
      ],
      "metadata": {
        "id": "X3BUbvIleKBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        xt = x.to(device, dtype=torch.float32)\n",
        "        yt = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(xt)\n",
        "        loss = loss_fn(y_pred, yt)\n",
        "        y_pred = torch.sigmoid(y_pred)\n",
        "\n",
        "        epoch_acc = give_acc(yt,y_pred)\n",
        "        epoch_acc += epoch_acc\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    Accuracy = epoch_acc/len(loader)\n",
        "    return epoch_loss, Accuracy\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            xt = x.to(device, dtype=torch.float32)\n",
        "            yt = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(xt)\n",
        "            loss = loss_fn(y_pred, yt)\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            epoch_acc = give_acc(yt,y_pred)\n",
        "            epoch_acc += epoch_acc\n",
        "\n",
        "        epoch_loss = epoch_loss/len(loader)\n",
        "        Accuracy = epoch_acc/len(loader)\n",
        "    return epoch_loss, Accuracy"
      ],
      "metadata": {
        "id": "KLyWYFSscQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model on CHASE dataset"
      ],
      "metadata": {
        "id": "uAZ0vd2HfxpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    seeding(42)\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Load dataset \"\"\"\n",
        "    train_x = sorted(glob(\"/content/new_data/train/image/*\"))\n",
        "    train_y = sorted(glob(\"/content/new_data/train/mask/*\"))\n",
        "\n",
        "    valid_x = sorted(glob(\"/content/new_data/test/image/*\"))\n",
        "    valid_y = sorted(glob(\"/content/new_data/test/mask/*\"))\n",
        "\n",
        "    data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
        "    print(data_str)\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    H = 512\n",
        "    W = 512\n",
        "    size = (H, W)\n",
        "    batch_size = 2\n",
        "    num_epochs = 40\n",
        "    lr = 1.3e-4\n",
        "    checkpoint_path = \"files/checkpoint.pth\"\n",
        "\n",
        "    \"\"\" Dataset and loader \"\"\"\n",
        "    train_dataset = DriveDataset(train_x, train_y)\n",
        "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "\n",
        "    train_loader = DataLoader( dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader( dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    \"\"\" Load model\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = build_unet()\n",
        "    model = model.to(device)\n",
        "\n",
        "    \"\"\" Model Parameters\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "    loss_fn = DiceBCELoss()\n",
        "\n",
        "    \"\"\" Training the model \"\"\"\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, loss_fn, device)\n",
        "        valid_loss, valid_acc = evaluate(model, valid_loader, loss_fn, device)\n",
        "\n",
        "        \"\"\" Saving the model \"\"\"\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "        data_str += f'\\tVal. loss: {valid_loss:.3f}\\n'\n",
        "        print(data_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8goxh4Hael1h",
        "outputId": "5f231838-0fd8-48c8-d87d-dbaa064e36d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size:\n",
            "Train: 80 - Valid: 20\n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 1.140\n",
            "\tVal. loss: 1.319\n",
            "\n",
            "Epoch: 02 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.959\n",
            "\tVal. loss: 0.970\n",
            "\n",
            "Epoch: 03 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.895\n",
            "\tVal. loss: 0.858\n",
            "\n",
            "Epoch: 04 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.841\n",
            "\tVal. loss: 0.811\n",
            "\n",
            "Epoch: 05 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.792\n",
            "\tVal. loss: 0.768\n",
            "\n",
            "Epoch: 06 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.748\n",
            "\tVal. loss: 0.711\n",
            "\n",
            "Epoch: 07 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.703\n",
            "\tVal. loss: 0.685\n",
            "\n",
            "Epoch: 08 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.665\n",
            "\tVal. loss: 0.653\n",
            "\n",
            "Epoch: 09 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.631\n",
            "\tVal. loss: 0.620\n",
            "\n",
            "Epoch: 10 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.598\n",
            "\tVal. loss: 0.585\n",
            "\n",
            "Epoch: 11 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.569\n",
            "\tVal. loss: 0.556\n",
            "\n",
            "Epoch: 12 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.544\n",
            "\tVal. loss: 0.545\n",
            "\n",
            "Epoch: 13 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.522\n",
            "\tVal. loss: 0.518\n",
            "\n",
            "Epoch: 14 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.502\n",
            "\tVal. loss: 0.501\n",
            "\n",
            "Epoch: 15 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.482\n",
            "\tVal. loss: 0.485\n",
            "\n",
            "Epoch: 16 | Epoch Time: 0m 32s\n",
            "\tTrain Loss: 0.465\n",
            "\tVal. loss: 0.472\n",
            "\n",
            "Epoch: 17 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.448\n",
            "\tVal. loss: 0.456\n",
            "\n",
            "Epoch: 18 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.436\n",
            "\tVal. loss: 0.438\n",
            "\n",
            "Epoch: 19 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.424\n",
            "\tVal. loss: 0.431\n",
            "\n",
            "Epoch: 20 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.411\n",
            "\tVal. loss: 0.422\n",
            "\n",
            "Epoch: 21 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.400\n",
            "\tVal. loss: 0.419\n",
            "\n",
            "Epoch: 22 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.391\n",
            "\tVal. loss: 0.406\n",
            "\n",
            "Epoch: 23 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.380\n",
            "\tVal. loss: 0.407\n",
            "\n",
            "Epoch: 24 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.375\n",
            "\tVal. loss: 0.405\n",
            "\n",
            "Epoch: 25 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.367\n",
            "\tVal. loss: 0.392\n",
            "\n",
            "Epoch: 26 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.355\n",
            "\tVal. loss: 0.387\n",
            "\n",
            "Epoch: 27 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.351\n",
            "\tVal. loss: 0.388\n",
            "\n",
            "Epoch: 28 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.341\n",
            "\tVal. loss: 0.382\n",
            "\n",
            "Epoch: 29 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.332\n",
            "\tVal. loss: 0.381\n",
            "\n",
            "Epoch: 30 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.325\n",
            "\tVal. loss: 0.385\n",
            "\n",
            "Epoch: 31 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.319\n",
            "\tVal. loss: 0.390\n",
            "\n",
            "Epoch: 32 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.314\n",
            "\tVal. loss: 0.377\n",
            "\n",
            "Epoch: 33 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.305\n",
            "\tVal. loss: 0.378\n",
            "\n",
            "Epoch: 34 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.298\n",
            "\tVal. loss: 0.381\n",
            "\n",
            "Epoch: 35 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.289\n",
            "\tVal. loss: 0.375\n",
            "\n",
            "Epoch: 36 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.284\n",
            "\tVal. loss: 0.378\n",
            "\n",
            "Epoch: 37 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.275\n",
            "\tVal. loss: 0.376\n",
            "\n",
            "Epoch: 38 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.270\n",
            "\tVal. loss: 0.376\n",
            "\n",
            "Epoch: 39 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.264\n",
            "\tVal. loss: 0.379\n",
            "\n",
            "Epoch: 40 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.258\n",
            "\tVal. loss: 0.375\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference code (Pass folder path and run)"
      ],
      "metadata": {
        "id": "ztkP8spEeQJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Seeding \"\"\"\n",
        "seeding(42)\n",
        "\n",
        "\"\"\" Folders \"\"\"\n",
        "create_dir(\"/content/drive/MyDrive/DIP/Retina_Images+Results\")\n",
        "create_dir(\"/content/drive/MyDrive/DIP/Retina_Results\")\n",
        "\n",
        "\"\"\" Load dataset \"\"\"\n",
        "images = sorted(glob(\"/content/drive/MyDrive/DIP/Retina_Images/*\")) ##Paste folder path before /*\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "H = 1024\n",
        "W = 1024\n",
        "size = (W, H)\n",
        "checkpoint_path = \"files/checkpoint.pth\"\n",
        "\n",
        "\"\"\" Load the checkpoint \"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = build_unet()\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "\n",
        "for i, x in tqdm(enumerate(zip(images)), total=len(images)):\n",
        "    \"\"\" Extract the name \"\"\"\n",
        "    name = x[0].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    \"\"\" Reading image \"\"\"\n",
        "    image = cv2.imread(x[0]) ## (512, 512, 3)\n",
        "    x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)\n",
        "    x = x/255.0\n",
        "    x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)\n",
        "    x = x.astype(np.float32)\n",
        "    x = torch.from_numpy(x)\n",
        "    x = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_y = model(x)\n",
        "        pred_y = torch.sigmoid(pred_y)\n",
        "\n",
        "        pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
        "        pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
        "        pred_y = pred_y > 0.5\n",
        "        pred_y = np.array(pred_y, dtype=np.uint8)\n",
        "\n",
        "    pred_y = mask_parse(pred_y)\n",
        "    line = np.ones((1024, 10, 3)) * 256\n",
        "\n",
        "    cat_images = np.concatenate(\n",
        "            [image, line, pred_y * 255], axis=1\n",
        "        )\n",
        "    cv2.imwrite(f\"/content/drive/MyDrive/DIP/Retina_Images+Results/{name}.png\", cat_images)\n",
        "    cv2.imwrite(f\"/content/drive/MyDrive/DIP/Retina_Results/{name}.png\", pred_y * 255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpfze2ofcQUV",
        "outputId": "92fbc85f-c755-4367-eb89-dd1948821a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:48<00:00,  1.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to calculate fractal dimension for a retinal vascular image\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.ndimage import maximum_filter\n",
        "from math import log, log2\n",
        "\n",
        "def fractal_dimension(image, max_iter=100):\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Perform maximum filtering to enhance the edges\n",
        "    filtered_image = maximum_filter(gray_image, size=3)\n",
        "\n",
        "    # Binarize the image\n",
        "    binary_image = (filtered_image > 0.5).astype(np.uint8)\n",
        "\n",
        "    # Calculate the number of pixels in each row and column\n",
        "    row_sums = np.sum(binary_image, axis=1)\n",
        "    col_sums = np.sum(binary_image, axis=0)\n",
        "\n",
        "    # Calculate the box-counting dimension\n",
        "    box_count = np.zeros(max_iter)\n",
        "    for i in range(max_iter):\n",
        "        box_count[i] = len(np.where(row_sums >= 2**i)[0]) * len(np.where(col_sums >= 2**i)[0])\n",
        "\n",
        "    # Calculate the slope of the log-log plot\n",
        "    log_box_count = np.log(box_count)\n",
        "    log_iterations = np.log(np.arange(max_iter))\n",
        "    slope, intercept = np.polyfit(log_iterations, log_box_count, 1)\n",
        "\n",
        "    # Return the fractal dimension\n",
        "    return slope\n"
      ],
      "metadata": {
        "id": "0XWgeUfWkZzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to perform box counting\n",
        "def box_counting(binary_image):\n",
        "    scales = []  # List to store grid sizes (scales)\n",
        "    counts = []  # List to store the number of covered boxes at each scale\n",
        "\n",
        "    # Loop over different grid sizes (scales)\n",
        "    for scale in range(1, min(binary_image.shape) // 2):\n",
        "        # Initialize count for this scale\n",
        "        count = 0\n",
        "\n",
        "        # Divide the image into boxes of size 'scale x scale'\n",
        "        for i in range(0, binary_image.shape[0], scale):\n",
        "            for j in range(0, binary_image.shape[1], scale):\n",
        "                # Check if the box contains any white pixel\n",
        "                if np.any(binary_image[i:i+scale, j:j+scale] == 255):\n",
        "                    count += 1\n",
        "\n",
        "        # Store scale and count\n",
        "        scales.append(scale)\n",
        "        counts.append(count)\n",
        "\n",
        "    return scales, counts\n",
        "\n",
        "# Load the segmented binary image (you can loop through your segmented images)\n",
        "image_path = '/content/drive/MyDrive/DIP/Retina_Results/DATA-01_dr.png'\n",
        "binary_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Perform box counting on the binary image\n",
        "scales, counts = box_counting(binary_image)\n",
        "\n",
        "# Convert lists to numpy arrays for calculations\n",
        "scales = np.array(scales)\n",
        "counts = np.array(counts)\n",
        "\n",
        "# Calculate the logarithms of scales and counts\n",
        "log_scales = np.log(scales)\n",
        "log_counts = np.log(counts)\n",
        "\n",
        "# Fit a linear regression line to the log-log data\n",
        "slope, intercept = np.polyfit(log_scales, log_counts, 1)\n",
        "\n",
        "# Calculate the fractal dimension\n",
        "fractal_dimension = -slope\n",
        "\n",
        "# Plot the log-log graph\n",
        "plt.figure()\n",
        "plt.scatter(log_scales, log_counts, marker='o', color='b', label='Box Count')\n",
        "plt.xlabel('Log Scale (log(1/box size))')\n",
        "plt.ylabel('Log Count (log(number of boxes))')\n",
        "plt.title(f'Fractal Dimension: {fractal_dimension:.2f}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot boxes on top of the original image\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(binary_image, cmap='gray')\n",
        "\n",
        "# Draw boxes on top of the image\n",
        "for i in range(0, binary_image.shape[0], int(scales[-1])):\n",
        "    for j in range(0, binary_image.shape[1], int(scales[-1])):\n",
        "        if np.any(binary_image[i:i+int(scales[-1]), j:j+int(scales[-1])] == 255):\n",
        "            rect = plt.Rectangle((j, i), int(scales[-1]), int(scales[-1]), linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f'Fractal Dimension: {fractal_dimension:.2f}')\n"
      ],
      "metadata": {
        "id": "9gpclQ6YP-GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to Store the Fractal Dimension of the images as a csv file\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Function to perform box counting\n",
        "def box_counting(binary_image):\n",
        "    scales = []  # List to store grid sizes (scales)\n",
        "    counts = []  # List to store the number of covered boxes at each scale\n",
        "\n",
        "    # Loop over different grid sizes (scales)\n",
        "    for scale in range(1, min(binary_image.shape) // 2):\n",
        "        # Initialize count for this scale\n",
        "        count = 0\n",
        "\n",
        "        # Divide the image into boxes of size 'scale x scale'\n",
        "        for i in range(0, binary_image.shape[0], scale):\n",
        "            for j in range(0, binary_image.shape[1], scale):\n",
        "                # Check if the box contains any white pixel\n",
        "                if np.any(binary_image[i:i+scale, j:j+scale] == 255):\n",
        "                    count += 1\n",
        "\n",
        "        # Store scale and count\n",
        "        scales.append(scale)\n",
        "        counts.append(count)\n",
        "\n",
        "    return scales, counts\n",
        "\n",
        "# Directory containing the image files\n",
        "images_folder = '/content/drive/MyDrive/DIP/Retina_Results/Retina_Images'\n",
        "\n",
        "# List to store fractal dimensions for each image\n",
        "fractal_dimensions = []\n",
        "\n",
        "# Iterate through all files in the folder\n",
        "for filename in os.listdir(images_folder):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # Construct the full path to the image\n",
        "        image_path = os.path.join(images_folder, filename)\n",
        "\n",
        "        # Load the segmented binary image\n",
        "        binary_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Perform box counting on the binary image\n",
        "        scales, counts = box_counting(binary_image)\n",
        "\n",
        "        # Convert lists to numpy arrays for calculations\n",
        "        scales = np.array(scales)\n",
        "        counts = np.array(counts)\n",
        "\n",
        "        # Calculate the logarithms of scales and counts\n",
        "        log_scales = np.log(scales)\n",
        "        log_counts = np.log(counts)\n",
        "\n",
        "        # Fit a linear regression line to the log-log data\n",
        "        slope, intercept = np.polyfit(log_scales, log_counts, 1)\n",
        "\n",
        "        # Calculate the fractal dimension\n",
        "        fractal_dimension = -slope\n",
        "\n",
        "        # Append the fractal dimension to the list\n",
        "        fractal_dimensions.append({'Image': filename, 'Fractal Dimension': fractal_dimension})\n",
        "\n",
        "# Write the fractal dimensions to a CSV file\n",
        "csv_file_path = 'FD_calculated.csv'\n",
        "fields = ['Image', 'Fractal Dimension']\n",
        "\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fields)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(fractal_dimensions)\n",
        "\n",
        "print(f'Fractal dimensions saved to {csv_file_path}')\n"
      ],
      "metadata": {
        "id": "rG9gvg_vP-Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to Visualize the FD's for Pathological and Non-Pathological images\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pathological_data = pd.read_csv('fractal_dimensions Provided.csv')\n",
        "non_pathological_data = pd.read_csv('fractal_dimensions.csv')\n",
        "\n",
        "\n",
        "# Plot the points\n",
        "plt.scatter(pathological_data['Fractal Dimension'], range(len(pathological_data)), color='red', label='Pathological')\n",
        "plt.scatter(non_pathological_data['Fractal Dimension'], range(len(non_pathological_data)), color='green', label='Non-Pathological')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Fractal Dimension')\n",
        "plt.ylabel('Data Point Index')\n",
        "plt.title('Fractal Dimension of Pathological and Non-Pathological Retinal Vascular Trees')\n",
        "\n",
        "# Show legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5Bsz3eRWQK9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}